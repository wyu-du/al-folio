---
---

@article{du-etal-2022-understanding-iterative,
  abbr={ACL},
  bibtex_show={true},
  title={Understanding Iterative Revision from Human-Written Text},
  author={Du, Wanyu and Raheja, Vipul and Kumar, Dhruv and Kim, Zae Myung and Lopez, Melissa and Kang, Dongyeop},
  journal={Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics},
  pages={3573--3590},
  year={2022},
  month={May},
  doi={10.18653/v1/2022.acl-long.250},
  url={https://aclanthology.org/2022.acl-long.250},
  html={https://aclanthology.org/2022.acl-long.250},
  publisher={Association for Computational Linguistics},
  abstract ={Writing is, by nature, a strategic, adaptive, and, more importantly, an iterative process. A crucial part of writing is editing and revising the text. Previous works on text revision have focused on defining edit intention taxonomies within a single domain or developing computational models with a single level of edit granularity, such as sentence-level edits, which differ from human{'}s revision cycles. This work describes IteraTeR: the first large-scale, multi-domain, edit-intention annotated corpus of iteratively revised text. In particular, IteraTeR is collected based on a new framework to comprehensively model the iterative text revisions that generalizes to a variety of domains, edit intentions, revision depths, and granularities. When we incorporate our annotated edit intentions, both generative and action-based text revision models significantly improve automatic evaluations. Through our work, we better understand the text revision process, making vital connections between edit intentions and writing quality, enabling the creation of diverse corpora to support computational modeling of iterative text revisions.},
}

@article{du-etal-2022-read,
  abbr={Workshop},
  bibtex_show={true},
  title={Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision},
  author={Du, Wanyu  and
      Kim, Zae Myung  and
      Raheja, Vipul  and
      Kumar, Dhruv  and
      Kang, Dongyeop},
  abstract={Revision is an essential part of the human writing process. It tends to be strategic, adaptive, and, more importantly, iterative in nature. Despite the success of large language models on text revision tasks, they are limited to non-iterative, one-shot revisions. Examining and evaluating the capability of large language models for making continuous revisions and collaborating with human writers is a critical step towards building effective writing assistants. In this work, we present a human-in-the-loop iterative text revision system, Read, Revise, Repeat (R3), which aims at achieving high quality text revisions with minimal human efforts by reading model-generated revisions and user feedbacks, revising documents, and repeating human-machine interactions. In R3, a text revision model provides text editing suggestions for human writers, who can accept or reject the suggested edits. The accepted edits are then incorporated into the model for the next iteration of document revision. Writers can therefore revise documents iteratively by interacting with the system and simply accepting/rejecting its suggested edits until the text revision model stops making further revisions or reaches a predefined maximum number of revisions. Empirical experiments show that R3 can generate revisions with comparable acceptance rate to human writers at early revision depths, and the human-machine interaction can get higher quality revisions with fewer iterations and edits. The collected human-model interaction dataset and system code are available at \url{https://github.com/vipulraheja/IteraTeR}. Our system demonstration is available at \url{https://youtu.be/lK08tIpEoaE}.},
  journal={Proceedings of the First Workshop on Intelligent and Interactive Writing Assistants (In2Writing 2022)},
  pages={96--108},
  year={2022},
  month={May},
  publisher={Association for Computational Linguistics},
  doi={10.18653/v1/2022.in2writing-1.14},
  url={https://aclanthology.org/2022.in2writing-1.14},
  html={https://aclanthology.org/2022.in2writing-1.14}
}

@article{du2022diverse,
  abbr={Workshop},
  bibtex_show={true},
  title={Diverse Text Generation via Variational Encoder-Decoder Models with Gaussian Process Priors},
  abstract={Generating high quality texts with high diversity is important for many NLG applications, but current methods mostly focus on building deterministic models to generate higher quality texts and do not provide many options for promoting diversity. In this work, we present a novel latent structured variable model to generate high quality texts by enriching contextual representation learning of encoder-decoder models. Specifically, we introduce a stochastic function to map deterministic encoder hidden states into random context variables. The proposed stochastic function is sampled from a Gaussian process prior to (1) provide infinite number of joint Gaussian distributions of random context variables (diversity-promoting) and (2) explicitly model dependency between context variables (accurate-encoding). To address the learning challenge of Gaussian processes, we propose an efficient variational inference approach to approximate the posterior distribution of random context variables. We evaluate our method in two typical text generation tasks: paraphrase generation and text style transfer. Experimental results on benchmark datasets demonstrate that our method improves the generation quality and diversity compared with other baselines.},
  author={Du, Wanyu and Zhao, Jianqiao and Wang, Liwei and Ji, Yangfeng},
  journal={arXiv preprint},
  year={2022},
  month={May},
  publisher={arXiv},
  url={https://arxiv.org/abs/2204.01227},
  html={https://arxiv.org/abs/2204.01227}
}


@article{du-ji-2021-sidecontrol-controlled,
  abbr={Findings},
  bibtex_show={true},
  title={{S}ide{C}ontrol: Controlled Open-domain Dialogue Generation via Additive Side Networks},
  author={Du, Wanyu and Ji, Yangfeng},
  abstract ={Transformer-based pre-trained language models boost the performance of open-domain dialogue systems. Prior works leverage Transformer-based pre-trained language models to generate texts with desired attributes in two general approaches: (1) gradient-based methods: updating all latent representations of pre-trained models with gradients from attribute models; (2) weighted-decoding methods: re-ranking beam candidates from pre-trained models with attribute functions. However, gradient-based methods lead to high computation cost and can easily get overfitted on small training sets, while weighted-decoding methods are inherently constrained by the low-variance high-bias pre-trained model. In this work, we propose a novel approach to control the generation of Transformer-based pre-trained language models: the SideControl framework, which leverages a novel control attributes loss to incorporate useful control signals, and is shown to perform well with very limited training samples. We evaluate our proposed method on two benchmark open-domain dialogue datasets, and results show that the SideControl framework has better controllability, higher generation quality and better sample-efficiency than existing gradient-based and weighted-decoding baselines.},
  journal={Findings of the Association for Computational Linguistics: EMNLP 2021},
  pages={2175--2194},
  year={2021},
  month={Nov},
  publisher={Association for Computational Linguistics},
  doi={10.18653/v1/2021.findings-emnlp.188},
  url={https://aclanthology.org/2021.findings-emnlp.188},
  html={https://aclanthology.org/2021.findings-emnlp.188}
}

@article{gehrmann-etal-2021-gem,
  abbr={Workshop},
  bibtex_show={true},
  title={The {GEM} Benchmark: Natural Language Generation, its Evaluation and Metrics},
  author={Gehrmann, et al.},
  abstract={We introduce GEM, a living benchmark for natural language Generation (NLG), its Evaluation, and Metrics. Measuring progress in NLG relies on a constantly evolving ecosystem of automated metrics, datasets, and human evaluation standards. Due to this moving target, new models often still evaluate on divergent anglo-centric corpora with well-established, but flawed, metrics. This disconnect makes it challenging to identify the limitations of current models and opportunities for progress. Addressing this limitation, GEM provides an environment in which models can easily be applied to a wide set of tasks and in which evaluation strategies can be tested. Regular updates to the benchmark will help NLG research become more multilingual and evolve the challenge alongside models. This paper serves as the description of the data for the 2021 shared task at the associated GEM Workshop.},
  journal={Proceedings of the 1st Workshop on Natural Language Generation, Evaluation, and Metrics (GEM 2021)},
  pages={96--120},
  year={2021},
  month={Aug},
  publisher={Association for Computational Linguistics},
  doi={10.18653/v1/2021.gem-1.10},
  url={https://aclanthology.org/2021.gem-1.10},
  html={https://aclanthology.org/2021.gem-1.10}
}


@article{schoch-etal-2021-contextualizing,
  abbr={INLG},
  bibtex_show={true},
  title={Contextualizing Variation in Text Style Transfer Datasets},
  author={Schoch, Stephanie  and
      Du, Wanyu  and
      Ji, Yangfeng},
  abstract={Text style transfer involves rewriting the content of a source sentence in a target style. Despite there being a number of style tasks with available data, there has been limited systematic discussion of how text style datasets relate to each other. This understanding, however, is likely to have implications for selecting multiple data sources for model training. While it is prudent to consider inherent stylistic properties when determining these relationships, we also must consider how a style is realized in a particular dataset. In this paper, we conduct several empirical analyses of existing text style datasets. Based on our results, we propose a categorization of stylistic and dataset properties to consider when utilizing or comparing text style datasets.},
  journal={Proceedings of the 14th International Conference on Natural Language Generation},
  pages={226--239},
  year={2021},
  month={Aug},
  publisher={Association for Computational Linguistics},
  url={https://aclanthology.org/2021.inlg-1.22},
  html={https://aclanthology.org/2021.inlg-1.22}
}


@article{du-ji-2021-sidecontrol-controlled,
  abbr={EMNLP},
  bibtex_show={true},
  title={An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation},
  author={Du, Wanyu and Ji, Yangfeng},
  abstract ={Generating paraphrases from given sentences involves decoding words step by step from a large vocabulary. To learn a decoder, supervised learning which maximizes the likelihood of tokens always suffers from the exposure bias. Although both reinforcement learning (RL) and imitation learning (IL) have been widely used to alleviate the bias, the lack of direct comparison leads to only a partial image on their benefits. In this work, we present an empirical study on how RL and IL can help boost the performance of generating paraphrases, with the pointer-generator as a base model. Experiments on the benchmark datasets show that (1) imitation learning is constantly better than reinforcement learning; and (2) the pointer-generator models with imitation learning outperform the state-of-the-art methods with a large margin.},
  journal={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)},
  pages={6012--6018},
  year={2019},
  month={Nov},
  publisher={Association for Computational Linguistics},
  doi={10.18653/v1/D19-1619},
  url={https://aclanthology.org/D19-1619},
  html={https://aclanthology.org/D19-1619}
}
