<!DOCTYPE html> <html lang="en"> <head> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Wanyu Du</title> <meta name="author" content="Wanyu Du"/> <meta name="description" content=""/> <meta name="keywords" content="wanyu du, uva ilp, academic-website"/> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"/> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="none" id="highlight_theme_light"/> <link rel="shortcut icon" href="/assets/img/favicon-32x32.png"/> <link rel="stylesheet" href="/assets/css/main.css"> <link rel="canonical" href="https://wyu-du.github.io/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"/> <script src="/assets/js/theme.js"></script> <script src="/assets/js/dark_mode.js"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <div class="navbar-brand social"> <a href="mailto:%77%64%35%6A%71@%76%69%72%67%69%6E%69%61.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=pmjB_ZQAAAAJ" title="Google Scholar"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/wyu-du" title="GitHub"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/wanyu-du-8759b921a" title="LinkedIn"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/WanyuDu" title="Twitter"><i class="fab fa-twitter"></i></a> </div> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item active"> <a class="nav-link" href="/">about<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications</a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects</a> </li> <div class="toggle-container"> <a id="light-toggle"> <i class="fas fa-moon"></i> <i class="fas fa-sun"></i> </a> </div> </ul> </div> </div> </nav> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title"> <span class="font-weight-bold">Wanyu</span> Du </h1> <p class="desc"></p> </header> <article> <div class="profile float-right"><figure> <picture> <source media="(max-width: 480px)" srcset="/assets/img/prof_pic-480.webp"/> <source media="(max-width: 800px)" srcset="/assets/img/prof_pic-800.webp"/> <source media="(max-width: 1400px)" srcset="/assets/img/prof_pic-1400.webp"/> <img class="img-fluid z-dept-1 rounded" src="/assets/img/prof_pic.jpg" alt="prof_pic.jpg"/> </picture> </figure> </div> <div class="clearfix"> <p>I am an applied scientist at <a href="https://aws.amazon.com/">Amazon Web Services (AWS)</a>, working with <a href="https://qiyanjun.github.io/Homepage/">Dr. Yanjun Qi</a> on trustworthy LLMs. I obtained my Ph.D. in <a href="https://engineering.virginia.edu/departments/computer-science">Computer Science</a> at <a href="https://www.virginia.edu/">University of Virginia</a> in 2024, advised by <a href="http://yangfengji.net/">Prof. Yangfeng Ji</a>.</p> <p>My research focuses on developing controllable human-AI collaborative text generation systems, which comprise three components:</p> <ul> <li><b>User Intention Modeling</b>: modeling user requirements with user intention taxonomy to control and evaluate LLMs (<a href="https://aclanthology.org/2022.acl-long.250/">ACL2022</a>, <a href="https://aclanthology.org/2022.emnlp-main.715/">EMNLP2022</a>). </li> <li><b>Controllable LLMs Development</b>: controlling the generation of LLMs through fine-tuning parameter-efficient modules (<a href="https://aclanthology.org/2021.findings-emnlp.188/">EMNLP2021</a>), modeling variational context variables (<a href="https://arxiv.org/abs/2204.01227">ACL-WS2022</a>), self-training (<a href="https://aclanthology.org/2022.findings-emnlp.201/">EMNLP2022</a>) and reinforcement learning (<a href="https://aclanthology.org/D19-1619/">EMNLP2019</a>, <a href="https://arxiv.org/abs/2311.00953">2023</a>).</li> <li><b>Human-AI Interaction Application</b>: developing human-AI collaborative writing systems (<a href="https://aclanthology.org/2022.in2writing-1.14/">ACL-WS2022</a>).</li> </ul> </div> <div class="news"> <h2>News</h2> <div class="table-responsive"> <table class="table table-sm table-borderless"> <tr> <th scope="row" style="width: 10%;">Jun 2024</th> <td> Start my applied scientist full-time at <a href="https://aws.amazon.com/">AWS</a>. </td> </tr> <tr> <th scope="row" style="width: 10%;">Apr 2024</th> <td> Pass my phd dissertation defense. </td> </tr> <tr> <th scope="row" style="width: 10%;">Nov 2023</th> <td> New paper “Blending Reward Functions via Few Expert Demonstrations for Faithful and Accurate Knowledge-Grounded Dialogue Generation” is out. [<a href="https://arxiv.org/abs/2311.00953">Paper</a>] </td> </tr> <tr> <th scope="row" style="width: 10%;">May 2023</th> <td> Start my applied scientist internship at <a href="https://aws.amazon.com/">AWS</a>. </td> </tr> <tr> <th scope="row" style="width: 10%;">Oct 2022</th> <td> 3 papers accepted to EMNLP. </td> </tr> <tr> <th scope="row" style="width: 10%;">Oct 2022</th> <td> New paper “Self-training with Two-phase Self-augmentation for Few-shot Dialogue Generation” is out. [<a href="https://aclanthology.org/2022.findings-emnlp.201/">Paper</a>][<a href="https://github.com/wyu-du/Self-Training-Dialogue-Generation">Code</a>] </td> </tr> <tr> <th scope="row" style="width: 10%;">Sep 2022</th> <td> 1 paper accepted to ACL, and 1 paper recieved the <strong>best paper award</strong> at <a href="https://in2writing.glitch.me/archive/2022/papers.html">ACL In2Writing Workshop</a>! </td> </tr> <tr> <th scope="row" style="width: 10%;">Apr 2022</th> <td> New paper “Read, Revise, Repeat: A System Demonstration for Human-in-the-loop Iterative Text Revision” is out. [<a href="https://aclanthology.org/2022.in2writing-1.14/">Paper</a>][<a href="https://github.com/vipulraheja/IteraTeR">Code</a>][<a href="https://aclanthology.org/2022.in2writing-1.14.mp4">Video</a>][<a href="https://www.youtube.com/watch?v=lK08tIpEoaE">Demo</a>] </td> </tr> <tr> <th scope="row" style="width: 10%;">Mar 2022</th> <td> New paper “Understanding Iterative Revision from Human-Written Text” is out. [<a href="https://aclanthology.org/2022.acl-long.250/">Paper</a>][<a href="https://github.com/vipulraheja/IteraTeR">Code</a>][<a href="https://aclanthology.org/2022.acl-long.250.mp4">Video</a>] </td> </tr> </table> </div> </div> <div class="publications"> <h2>Selected publications</h2> <ol class="bibliography"><li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="du-etal-2022-sa2" class="col-sm-8"> <div class="title">Self-training with Two-phase Self-augmentation for Few-shot Dialogue Generation</div> <div class="author"> <em>Du, Wanyu</em>,&nbsp;Chen, Hanjie,&nbsp;and Ji, Yangfeng </div> <div class="periodical"> <em>Findings of the Association for Computational Linguistics: EMNLP 2022</em> Dec 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.findings-emnlp.201.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/wyu-du/Self-Training-Dialogue-Generation" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>In task-oriented dialogue systems, response generation from meaning representations (MRs) often suffers from limited training examples, due to the high cost on annotating MR-to-Text pairs. Previous works on self-training leverage fine-tuned conversational models to automatically generate pseudo-labeled MR-to-Text pairs for further fine-tuning. However, some self-augmented data may be noisy or uninformative for the model to learn from. In this work, we propose a two-phase self-augmentation procedure to generate high-quality pseudo-labeled MR-to-Text pairs: the first phase selects the most informative MRs based on model’s prediction uncertainty; with the selected MRs, the second phase generates accurate responses by aggregating multiple perturbed latent representations from each MR. Empirical experiments on two benchmark datasets, FewShotWOZ and FewShotSGD, show that our method generally outperforms existing self-training methods on both automatic and human evaluations.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">ACL</abbr></div> <div id="du-etal-2022-understanding-iterative" class="col-sm-8"> <div class="title">Understanding Iterative Revision from Human-Written Text</div> <div class="author"> <em>Du, Wanyu</em>,&nbsp;Raheja, Vipul,&nbsp;Kumar, Dhruv,&nbsp;Kim, Zae Myung,&nbsp;Lopez, Melissa,&nbsp;and Kang, Dongyeop </div> <div class="periodical"> <em>Proceedings of the 60th Annual Meeting of the Association for Computational Linguistics</em> May 2022 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2022.acl-long.250.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://colab.research.google.com/drive/1qv7b2jJSqqMaYOQ5NRvAvoyDB3gvpwcp?usp=sharing" class="btn btn-sm z-depth-0" role="button">Demo</a> <a href="https://github.com/vipulraheja/IteraTeR" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Writing is, by nature, a strategic, adaptive, and, more importantly, an iterative process. A crucial part of writing is editing and revising the text. Previous works on text revision have focused on defining edit intention taxonomies within a single domain or developing computational models with a single level of edit granularity, such as sentence-level edits, which differ from human’s revision cycles. This work describes IteraTeR: the first large-scale, multi-domain, edit-intention annotated corpus of iteratively revised text. In particular, IteraTeR is collected based on a new framework to comprehensively model the iterative text revisions that generalizes to a variety of domains, edit intentions, revision depths, and granularities. When we incorporate our annotated edit intentions, both generative and action-based text revision models significantly improve automatic evaluations. Through our work, we better understand the text revision process, making vital connections between edit intentions and writing quality, enabling the creation of diverse corpora to support computational modeling of iterative text revisions.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="du-ji-2021-sidecontrol-controlled" class="col-sm-8"> <div class="title">SideControl: Controlled Open-domain Dialogue Generation via Additive Side Networks</div> <div class="author"> <em>Du, Wanyu</em>,&nbsp;and Ji, Yangfeng </div> <div class="periodical"> <em>Findings of the Association for Computational Linguistics: EMNLP 2021</em> Nov 2021 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/2021.findings-emnlp.188.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/wyu-du/Controlled-Dialogue-Generation" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Transformer-based pre-trained language models boost the performance of open-domain dialogue systems. Prior works leverage Transformer-based pre-trained language models to generate texts with desired attributes in two general approaches: (1) gradient-based methods: updating all latent representations of pre-trained models with gradients from attribute models; (2) weighted-decoding methods: re-ranking beam candidates from pre-trained models with attribute functions. However, gradient-based methods lead to high computation cost and can easily get overfitted on small training sets, while weighted-decoding methods are inherently constrained by the low-variance high-bias pre-trained model. In this work, we propose a novel approach to control the generation of Transformer-based pre-trained language models: the SideControl framework, which leverages a novel control attributes loss to incorporate useful control signals, and is shown to perform well with very limited training samples. We evaluate our proposed method on two benchmark open-domain dialogue datasets, and results show that the SideControl framework has better controllability, higher generation quality and better sample-efficiency than existing gradient-based and weighted-decoding baselines.</p> </div> </div> </div> </li> <li> <div class="row"> <div class="col-sm-2 abbr"><abbr class="badge">EMNLP</abbr></div> <div id="du-ji-2021-sidecontrol-controllee" class="col-sm-8"> <div class="title">An Empirical Comparison on Imitation Learning and Reinforcement Learning for Paraphrase Generation</div> <div class="author"> <em>Du, Wanyu</em>,&nbsp;and Ji, Yangfeng </div> <div class="periodical"> <em>Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)</em> Nov 2019 </div> <div class="links"> <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a> <a href="https://aclanthology.org/D19-1619.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a> <a href="https://github.com/wyu-du/Reinforce-Paraphrase-Generation" class="btn btn-sm z-depth-0" role="button">Code</a> </div> <div class="abstract hidden"> <p>Generating paraphrases from given sentences involves decoding words step by step from a large vocabulary. To learn a decoder, supervised learning which maximizes the likelihood of tokens always suffers from the exposure bias. Although both reinforcement learning (RL) and imitation learning (IL) have been widely used to alleviate the bias, the lack of direct comparison leads to only a partial image on their benefits. In this work, we present an empirical study on how RL and IL can help boost the performance of generating paraphrases, with the pointer-generator as a base model. Experiments on the benchmark datasets show that (1) imitation learning is constantly better than reinforcement learning; and (2) the pointer-generator models with imitation learning outperform the state-of-the-art methods with a large margin.</p> </div> </div> </div> </li></ol> </div> <div class="social"> <div class="contact-icons"> <a href="mailto:%77%64%35%6A%71@%76%69%72%67%69%6E%69%61.%65%64%75" title="email"><i class="fas fa-envelope"></i></a> <a href="https://scholar.google.com/citations?user=pmjB_ZQAAAAJ" title="Google Scholar"><i class="ai ai-google-scholar"></i></a> <a href="https://github.com/wyu-du" title="GitHub"><i class="fab fa-github"></i></a> <a href="https://www.linkedin.com/in/wanyu-du-8759b921a" title="LinkedIn"><i class="fab fa-linkedin"></i></a> <a href="https://twitter.com/WanyuDu" title="Twitter"><i class="fab fa-twitter"></i></a> </div> <div class="contact-note"> </div> </div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> &copy; Copyright 2024 Wanyu Du. Powered by <a href="https://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>. Last updated: June 17, 2024. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/@popperjs/core@2.11.2/dist/umd/popper.min.js" integrity="sha256-l/1pMF/+J4TThfgARS6KwWrk/egwuVvhRzfLAMQ6Ds4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.min.js" integrity="sha256-SyTu6CwrfOhaznYZPoolVw2rxoY7lKYKQvqbtqN93HI=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script> <script src="/assets/js/zoom.js"></script> <script src="/assets/js/common.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> </body> </html>